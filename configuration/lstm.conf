[encoder]
layers_num = 2
num_units = [256, 256]
forget_bias = [1.0, 1.0]
state_is_tuple= [True, True]
activation = [None, None]
reuse = [None, None]
name = [None, None]
dropout = [True, True]
input_keep_prob = [1.0, 1.0]
output_keep_prob = [1.0, 1.0]
state_keep_prob = [1.0, 1.0]
variational_recurrent = [False, False]
input_size = [None, None]
dtype = [None, None]
seed = [None, None]
dropout_state_filter_visitor = [None, None]
multi_state_is_tuple = True
sequence_length = None
initial_state = None
dynamic_rnn_dtype = 'float32'
parallel_iterations = None
swap_memory = False
time_major = False
scope = None

[fullyconnected]
activation_fn = None
normalizer_fn = None
normalizer_params = None
weights_initializer = 'truncated_normal'
weights_initializer_stddev = 0.1
weights_regularizer = None
biases_initializer = 'zeros'
biases_regularizer = None
reuse = None
variables_collections = None
outputs_collections = None
trainable = True
scope = None


[loss]
loss = 'cross_entropy'
dim = -1
name = None

[optimizer]
optimizer = 'GradientDescent'
use_locking = False
name = 'GradientDescent'

[accuracy]
accuracy = 'argmax'
axis = 1

[saver]
max_to_keep = 5
model_name = 'cnn'

[train]
batch_size = 1000
init_learning_rate = 0.001
max_epoch = 1000
expected_accuracy = 1.0
accuracy_every_epoch = 10
save_every_epoch = 10